# Resource Monitoring

## Feature Overview

Comprehensive resource monitoring for database containers, tracking CPU, memory, disk I/O, and network usage. Provides real-time metrics collection, historical data retention, and the foundation for performance profiling and comparison across database configurations.

**Status:** Implemented. CLI command name is `dbarena` (legacy examples may still show `simdb`). See `specs/IMPLEMENTATION_TRUTH.md`.

## Problem Statement

When testing databases, understanding resource consumption is critical:
- Which database type is most memory-efficient for a given workload?
- Is the database CPU-bound or I/O-bound?
- Are there memory leaks during extended test runs?
- How much network bandwidth does replication consume?

Without resource monitoring, users must rely on external tools (docker stats, htop) and manually correlate data with database instances. This creates friction and makes systematic comparisons difficult.

## User Stories

**As a performance engineer**, I want to:
- See real-time CPU and memory usage for each database container
- Compare resource consumption across PostgreSQL, MySQL, and SQL Server
- Identify when a database is approaching resource limits
- Export resource metrics for analysis in external tools

**As a CDC developer**, I want to:
- Measure the resource overhead of enabling CDC features
- Monitor network traffic generated by replication
- Detect memory leaks during long-running CDC tests
- Understand disk I/O patterns when change events are captured

**As a DevOps engineer**, I want to:
- Set resource limits (CPU, memory) for containers
- Receive warnings when containers approach limits
- Understand baseline resource requirements for capacity planning
- Track resource usage trends over time

## Technical Requirements

### Functional Requirements

**FR-1: CPU Metrics**
- CPU usage percentage (0-100% per core, or aggregate across cores)
- CPU time (user, system, total)
- CPU throttling events (when container hits CPU limits)
- Update frequency: 1 second

**FR-2: Memory Metrics**
- Current memory usage (RSS, cache, swap)
- Memory limit (if set)
- Memory usage percentage
- Page faults (major and minor)
- Update frequency: 1 second

**FR-3: Disk I/O Metrics**
- Read operations (count and bytes)
- Write operations (count and bytes)
- I/O wait time
- Throughput (MB/s for reads and writes)
- Update frequency: 1 second

**FR-4: Network Metrics**
- Bytes received (inbound)
- Bytes transmitted (outbound)
- Packets received/transmitted
- Network throughput (MB/s)
- Update frequency: 1 second

**FR-5: Historical Data**
- Retain metrics for configurable duration (default: 1 hour)
- Downsample older data (1-minute averages for data >10 minutes old)
- Export historical data to JSON/CSV
- Clear historical data on container destruction

**FR-6: Resource Limits**
- Set container CPU limits (number of cores or percentage)
- Set container memory limits (MB or GB)
- Enforce limits via Docker API
- Display current limits in metrics output

**FR-7: Alerting**
- Configurable threshold warnings (e.g., >80% memory usage)
- Log warnings to stdout/stderr
- Visual indicators in TUI (future: v0.4.0)
- No silent failures: always report monitoring errors

### Non-Functional Requirements

**NFR-1: Performance**
- Metrics collection overhead <5% CPU, <20MB RAM per monitored container
- Metrics retrieval latency <50ms
- Historical query response <200ms for 1 hour of data

**NFR-2: Accuracy**
- Metrics accurate within ±5% of actual resource usage
- Timestamps synchronized with system clock
- No data loss during normal operations

**NFR-3: Reliability**
- Handle Docker daemon restarts gracefully
- Reconnect to containers after network interruptions
- Continue monitoring existing containers after simDB CLI restart

## Architecture & Design

### Components

```
Docker Stats API
    ↓
Metrics Collector (polling loop)
    ↓
Metrics Storage (in-memory time-series)
    ↓
Metrics Query API
    ↓
CLI Display / Export
```

### Key Modules

**`monitoring/metrics_collector.rs`**
- `MetricsCollector`: Background task for polling Docker stats
- Collects metrics for all simDB-managed containers
- Stores metrics in time-series data structure
- Handles Docker API errors and retries

**`monitoring/metrics_storage.rs`**
- `MetricsStorage`: In-memory time-series storage
- Ring buffer for fixed-size historical data
- Query interface for retrieving metrics by time range
- Downsampling for older data

**`monitoring/resource_limits.rs`**
- `ResourceLimits`: Configure container resource limits
- Apply limits via Docker API (update container config)
- Validate limit values before applying

**`monitoring/models.rs`**
- `ResourceMetrics`: Struct containing all resource metrics
- `CpuStats`, `MemoryStats`, `DiskIoStats`, `NetworkStats` sub-structs
- Serialization for JSON export

### Data Flow

1. **Metrics Collection Loop:**
   ```
   Every 1 second:
   → Query Docker stats API for all containers
   → Parse stats response
   → Store in MetricsStorage
   → Check threshold alerts
   → Log warnings if needed
   ```

2. **Metrics Query:**
   ```
   User request → MetricsStorage.query(time_range) →
   Retrieve data points → Aggregate if needed → Return
   ```

## CLI Interface Design

### Commands

```bash
# Show current resource usage for all containers
simdb stats [--json] [--interval <seconds>]

# Show detailed stats for a specific container
simdb stats <name> [--json] [--interval <seconds>]

# Show historical resource usage
simdb stats <name> --history [--duration <minutes>] [--format <json|csv>]

# Set resource limits
simdb limits <name> --cpu <cores> --memory <MB>

# Export metrics
simdb export-metrics <name> --output <file> [--format <json|csv>]
```

### Example Usage

```bash
# Show current stats for all containers
$ simdb stats
NAME                      CPU %    MEMORY         DISK I/O       NETWORK
simdb-postgres-16-a3f9    2.5%     128MB / 2GB    10MB / 5MB     1.2KB / 0.8KB
simdb-mysql-8-b7e2        3.1%     256MB / 2GB    15MB / 8MB     2.1KB / 1.5KB

# Show live stats with 2-second updates
$ simdb stats simdb-postgres-16-a3f9 --interval 2
Container: simdb-postgres-16-a3f9
  CPU Usage:     2.5% (0.025 cores)
  Memory:        128MB / 2GB (6.4%)
  Disk Read:     10MB (250 ops)
  Disk Write:    5MB (180 ops)
  Network In:    1.2KB
  Network Out:   0.8KB

  CPU Usage:     2.8% (0.028 cores)
  Memory:        130MB / 2GB (6.5%)
  Disk Read:     11MB (265 ops)
  Disk Write:    5.5MB (195 ops)
  Network In:    1.5KB
  Network Out:   1.0KB

# Set resource limits
$ simdb limits simdb-postgres-16-a3f9 --cpu 2 --memory 1024
Resource limits updated:
  CPU Limit: 2 cores
  Memory Limit: 1024MB

# Export historical metrics (last 30 minutes)
$ simdb export-metrics simdb-postgres-16-a3f9 --output metrics.json --duration 30
Exported 1800 data points to metrics.json
```

### JSON Output Format

```json
{
  "container_name": "simdb-postgres-16-a3f9",
  "timestamp": "2026-01-22T14:30:45Z",
  "cpu": {
    "usage_percent": 2.5,
    "cores_used": 0.025,
    "user_cpu_time_ms": 12500,
    "system_cpu_time_ms": 3200
  },
  "memory": {
    "usage_bytes": 134217728,
    "limit_bytes": 2147483648,
    "usage_percent": 6.25,
    "cache_bytes": 25165824,
    "swap_bytes": 0
  },
  "disk_io": {
    "read_bytes": 10485760,
    "read_ops": 250,
    "write_bytes": 5242880,
    "write_ops": 180,
    "read_throughput_mbps": 0.5,
    "write_throughput_mbps": 0.25
  },
  "network": {
    "rx_bytes": 1228,
    "tx_bytes": 819,
    "rx_packets": 15,
    "tx_packets": 12,
    "rx_throughput_kbps": 9.6,
    "tx_throughput_kbps": 6.4
  }
}
```

## Implementation Details

### Dependencies (Rust Crates)

```toml
[dependencies]
bollard = "0.16"               # Docker API client
tokio = { version = "1.36", features = ["full"] }
serde = { version = "1.0", features = ["derive"] }
serde_json = "1.0"
csv = "1.3"                    # CSV export
chrono = "0.4"                 # Timestamp handling
parking_lot = "0.12"           # Efficient locks for metrics storage
```

### Metrics Collection Implementation

```rust
use bollard::Docker;
use tokio::time::{interval, Duration};

pub struct MetricsCollector {
    docker: Docker,
    storage: Arc<MetricsStorage>,
    poll_interval: Duration,
}

impl MetricsCollector {
    pub async fn start(&self) {
        let mut ticker = interval(self.poll_interval);

        loop {
            ticker.tick().await;

            match self.collect_metrics().await {
                Ok(metrics_map) => {
                    for (container_name, metrics) in metrics_map {
                        self.storage.store(container_name, metrics);
                    }
                }
                Err(e) => {
                    tracing::warn!("Failed to collect metrics: {}", e);
                }
            }
        }
    }

    async fn collect_metrics(&self) -> Result<HashMap<String, ResourceMetrics>> {
        let containers = self.list_simdb_containers().await?;
        let mut metrics_map = HashMap::new();

        for container in containers {
            let stats = self.docker
                .stats(&container.id, Some(StatsOptions { stream: false, ..Default::default() }))
                .try_collect::<Vec<_>>()
                .await?;

            if let Some(stats) = stats.first() {
                let metrics = self.parse_stats(stats)?;
                metrics_map.insert(container.name.clone(), metrics);
            }
        }

        Ok(metrics_map)
    }

    fn parse_stats(&self, stats: &Stats) -> Result<ResourceMetrics> {
        // Parse CPU stats
        let cpu_delta = stats.cpu_stats.cpu_usage.total_usage
            - stats.precpu_stats.cpu_usage.total_usage;
        let system_delta = stats.cpu_stats.system_cpu_usage
            .unwrap_or(0)
            - stats.precpu_stats.system_cpu_usage.unwrap_or(0);

        let cpu_percent = if system_delta > 0 {
            (cpu_delta as f64 / system_delta as f64)
                * stats.cpu_stats.online_cpus.unwrap_or(1) as f64
                * 100.0
        } else {
            0.0
        };

        // Parse memory stats
        let memory_usage = stats.memory_stats.usage.unwrap_or(0);
        let memory_limit = stats.memory_stats.limit.unwrap_or(u64::MAX);
        let memory_percent = (memory_usage as f64 / memory_limit as f64) * 100.0;

        // Parse disk I/O
        let disk_read = stats.blkio_stats.io_service_bytes_recursive
            .iter()
            .flatten()
            .filter(|s| s.op == "read")
            .map(|s| s.value)
            .sum::<u64>();

        let disk_write = stats.blkio_stats.io_service_bytes_recursive
            .iter()
            .flatten()
            .filter(|s| s.op == "write")
            .map(|s| s.value)
            .sum::<u64>();

        // Parse network stats
        let (rx_bytes, tx_bytes) = stats.networks
            .as_ref()
            .map(|nets| {
                let rx = nets.values().map(|n| n.rx_bytes).sum::<u64>();
                let tx = nets.values().map(|n| n.tx_bytes).sum::<u64>();
                (rx, tx)
            })
            .unwrap_or((0, 0));

        Ok(ResourceMetrics {
            timestamp: chrono::Utc::now(),
            cpu: CpuStats {
                usage_percent: cpu_percent,
                cores_used: cpu_percent / 100.0,
            },
            memory: MemoryStats {
                usage_bytes: memory_usage,
                limit_bytes: memory_limit,
                usage_percent: memory_percent,
            },
            disk_io: DiskIoStats {
                read_bytes: disk_read,
                write_bytes: disk_write,
            },
            network: NetworkStats {
                rx_bytes,
                tx_bytes,
            },
        })
    }
}
```

### Time-Series Storage

```rust
use std::collections::VecDeque;

pub struct MetricsStorage {
    data: RwLock<HashMap<String, VecDeque<ResourceMetrics>>>,
    max_points: usize,
    retention_duration: Duration,
}

impl MetricsStorage {
    pub fn new(max_points: usize, retention_duration: Duration) -> Self {
        Self {
            data: RwLock::new(HashMap::new()),
            max_points,
            retention_duration,
        }
    }

    pub fn store(&self, container_name: String, metrics: ResourceMetrics) {
        let mut data = self.data.write();
        let series = data.entry(container_name).or_insert_with(VecDeque::new);

        // Add new data point
        series.push_back(metrics);

        // Trim old data
        while series.len() > self.max_points {
            series.pop_front();
        }

        // Remove data older than retention duration
        let cutoff = chrono::Utc::now() - self.retention_duration;
        while let Some(front) = series.front() {
            if front.timestamp < cutoff {
                series.pop_front();
            } else {
                break;
            }
        }
    }

    pub fn query(&self, container_name: &str, duration: Duration) -> Vec<ResourceMetrics> {
        let data = self.data.read();
        let series = match data.get(container_name) {
            Some(s) => s,
            None => return vec![],
        };

        let cutoff = chrono::Utc::now() - duration;
        series
            .iter()
            .filter(|m| m.timestamp >= cutoff)
            .cloned()
            .collect()
    }

    pub fn latest(&self, container_name: &str) -> Option<ResourceMetrics> {
        let data = self.data.read();
        data.get(container_name)?.back().cloned()
    }
}
```

## Testing Strategy

### Unit Tests

- `test_metrics_parsing()`: Verify Docker stats parsing produces correct metrics
- `test_cpu_calculation()`: Test CPU percentage calculation with various inputs
- `test_memory_calculation()`: Test memory usage calculations
- `test_storage_retention()`: Verify old data is properly trimmed
- `test_storage_query()`: Test querying metrics by time range

### Integration Tests

- `test_metrics_collection_postgres()`: Collect metrics from running PostgreSQL container
- `test_metrics_collection_mysql()`: Collect metrics from running MySQL container
- `test_resource_limits()`: Set limits, verify Docker API updates container
- `test_export_json()`: Export metrics, verify JSON format
- `test_export_csv()`: Export metrics, verify CSV format

### Performance Tests

- `test_collection_overhead()`: Measure CPU/memory overhead of metrics collection
- `test_storage_memory()`: Verify memory usage with 1 hour of metrics
- `test_query_performance()`: Measure query response time for large datasets

### Manual Testing Scenarios

1. **Live Monitoring**: Run `simdb stats --interval 1`, observe real-time updates
2. **Resource Stress**: Run CPU/memory-intensive query, verify metrics spike
3. **Historical Data**: Collect metrics for 1 hour, query various time ranges
4. **Container Restart**: Stop/start container, verify metrics resume
5. **Limit Enforcement**: Set low memory limit, trigger OOM, verify enforcement

## Documentation Requirements

- **Metrics Reference**: Complete list of available metrics with descriptions
- **Export Formats**: JSON and CSV format specifications
- **Resource Limits Guide**: How to set and manage container resource limits
- **Performance Tuning**: Best practices for minimizing monitoring overhead
- **Troubleshooting**: Common issues (missing metrics, inaccurate data)

## Migration/Upgrade Notes

Users of v0.1.0-v0.2.0:
- Metrics collection starts automatically for all existing containers
- No configuration changes required
- New CLI commands available: `stats`, `limits`, `export-metrics`

## Future Enhancements

- Prometheus metrics exporter
- Grafana dashboard templates
- Customizable alert rules (email, webhooks)
- Resource usage forecasting
- Comparative metrics (side-by-side container comparison)
- Database-specific metrics (query performance, connection counts) - coming in v0.4.0
